#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Mar  1 03:50:54 2023

@author: juwita
"""


import tensorflow as tf 
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, UpSampling2D
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Multiply, LeakyReLU
import numpy as np 
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import Recall, Precision 
from tensorflow.keras import backend as K 
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dropout, MaxPooling2D
from tensorflow.keras.utils import plot_model
import visualkeras





print("TF Version: ", tf.__version__)

#---------------------------------Seeding-----------------------------#
np.random.seed(42)
tf.random.set_seed(42)



#---------------------------------Hyperparameter-----------------------------#




def spatial_attention(x):
    #average pooling 
    x1 = tf.reduce_mean(x, axis = -1) #ini dari yt, untuk ambil dua dimensi saja? 
    x1 = tf.expand_dims (x1, axis = -1) # dari yt(idiot..) 
    
    #MaxPooling
    x2 = tf.reduce_max (x, axis = -1)
    x2 = tf.expand_dims(x2, axis = -1 )
   
    #concatenate 
    comb = Concatenate()([x1,x2])
    
    #convLayer 
    comb = Conv2D(1, kernel_size = 3, padding="same", activation = "sigmoid")(comb)
    comb = Multiply()([x,comb])
    
    return comb
    

def combine_attention(x):
    x1 = spatial_attention(x)
    f_a = x1
    f_b = Conv2D(1, (1, 1), padding='same', use_bias=False)(x)
    f_act = tf.keras.layers.Activation('sigmoid')(f_b)
    
    combined = Concatenate()([f_act, f_a])
      
   
    combined = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')(combined) #ini sudah daku ubah harusnya conv2D(1,)
    combined = Multiply()([x, combined])
    
  
    
    return combined




#---------------------------------Model -----------------------------#

def build_model(H,W,C, encoder_trainable=False): 
    inputs = Input(shape=(H, W, C), name = "input_image")
    
    
    #preTrained encoder 
    encoder = MobileNetV2(include_top=False, weights="imagenet",
        input_tensor=inputs)
       
    encoder.trainable = encoder_trainable
          
    skip_connection_names = ["input_image", "block_1_expand_relu", "block_3_expand_relu", "block_6_expand_relu"]
   
    encoder_output = encoder.get_layer ("block_13_expand_relu").output  #(16x16) 
   
    for name in skip_connection_names:
       layer_output = encoder.get_layer(name).output
       print("Shape of the output for layer", name, ":", layer_output.shape)
    
  
    f = [16,32,64,128]
    x =encoder_output
    x = combine_attention(x)
    
    encoder.summary()
    print (x.shape)
    
    for i in range(1,len(f)+1, 1):
        print(i)
        x_skip = encoder.get_layer(skip_connection_names[-i]).output 
        print(x_skip)
        x = UpSampling2D((2,2))(x)
        x = Concatenate()([x,x_skip])
        
        x = combine_attention(x)
       
       
        x = Dropout(0.2)(x)
        
        x = Conv2D(f[-i],(3,3),padding = "same")(x)
        x = BatchNormalization()(x)
        x = Activation("relu")(x)
        #x = Dropout(0.2)(x)
        
        
        x = Conv2D(f[-i],(3,3),padding = "same")(x)
        x = BatchNormalization()(x)
        x = Activation("relu")(x)
        #x = Dropout(0.5)(x)
        
    #output
    x = Conv2D(3, (1,1), padding="same")(x)
    x = Activation("sigmoid")(x)
    
    model = Model(inputs,x)
    return model 


   
    




if __name__ == "__main__":
    H = 64
    W =64
    C= 3
    input_shape = 64
    model = build_model(H,W,C)
    model.summary()
    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)
    #visualkeras.layered_view(model, to_file='model_plot.png')
    #execute 
    # inputs = Input(shape = (128,128,1))
    # inputss = (128,128,1)
    # y = Quick_Spatial(inputs)
    # print ((y.shape))

   

